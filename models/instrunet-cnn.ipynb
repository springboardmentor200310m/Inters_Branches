{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-19T12:13:49.658957Z",
     "iopub.status.busy": "2025-12-19T12:13:49.658387Z",
     "iopub.status.idle": "2025-12-19T12:13:55.741629Z",
     "shell.execute_reply": "2025-12-19T12:13:55.741004Z",
     "shell.execute_reply.started": "2025-12-19T12:13:49.658927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'Accordion': 0, 'Acoustic_Guitar': 1, 'Banjo': 2, 'Bass_Guitar': 3, 'Clarinet': 4, 'Cymbals': 5, 'Dobro': 6, 'Drum_set': 7, 'Electro_Guitar': 8, 'Floor_Tom': 9, 'Harmonica': 10, 'Harmonium': 11, 'Hi_Hats': 12, 'Horn': 13, 'Keyboard': 14, 'Mandolin': 15, 'Organ': 16, 'Piano': 17, 'Saxophone': 18, 'Shakers': 19, 'Tambourine': 20, 'Trombone': 21, 'Trumpet': 22, 'Ukulele': 23, 'Violin': 24, 'cowbell': 25, 'flute': 26, 'vibraphone': 27}\n",
      "Total files: 42311\n",
      "Train size: 33848 Val size: 8463\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "import numpy as np\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/music-instrunment-sounds-for-classification\")  # adjust if needed\n",
    "TRAIN_DIR = DATA_ROOT / \"music_dataset\"   # check exact name on Kaggle UI\n",
    "VAL_RATIO = 0.2                   # use 20% of train as validation if no val folder\n",
    "\n",
    "def build_file_list(root_dir):\n",
    "    wav_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted([d.name for d in root_dir.iterdir() if d.is_dir()])\n",
    "    class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "    for cls in class_names:\n",
    "        for wav in (root_dir / cls).glob(\"*.wav\"):\n",
    "            wav_paths.append(str(wav))\n",
    "            labels.append(class_to_idx[cls])\n",
    "    return wav_paths, labels, class_to_idx\n",
    "\n",
    "all_paths, all_labels, class_to_idx = build_file_list(TRAIN_DIR)\n",
    "print(\"Classes:\", class_to_idx)\n",
    "print(\"Total files:\", len(all_paths))\n",
    "\n",
    "# train/val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    all_paths, all_labels, test_size=VAL_RATIO, stratify=all_labels, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_paths), \"Val size:\", len(val_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T12:14:00.607439Z",
     "iopub.status.busy": "2025-12-19T12:14:00.606490Z",
     "iopub.status.idle": "2025-12-19T12:14:02.585579Z",
     "shell.execute_reply": "2025-12-19T12:14:02.584707Z",
     "shell.execute_reply.started": "2025-12-19T12:14:00.607399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 1, 128, 128])\n",
      "Labels: tensor([15, 21, 16,  3,  0,  1,  3, 26])\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 256\n",
    "SPEC_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "class InstrumentDataset(Dataset):\n",
    "    def __init__(self, paths, labels):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "\n",
    "        self.mel = MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            n_mels=N_MELS,\n",
    "            center=True,\n",
    "            power=2.0,\n",
    "        )\n",
    "        self.to_db = AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _fix_length(self, spec):\n",
    "        if spec.size(1) < SPEC_LEN:\n",
    "            pad = SPEC_LEN - spec.size(1)\n",
    "            spec = torch.nn.functional.pad(spec, (0, pad))\n",
    "        elif spec.size(1) > SPEC_LEN:\n",
    "            spec = spec[:, :SPEC_LEN]\n",
    "        return spec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        spec = self.mel(waveform)\n",
    "        spec = self.to_db(spec).squeeze(0)\n",
    "        spec = self._fix_length(spec)\n",
    "\n",
    "        mean = spec.mean()\n",
    "        std = spec.std() + 1e-6\n",
    "        spec = (spec - mean) / std\n",
    "        spec = spec.unsqueeze(0)   # (1, 128, 128)\n",
    "\n",
    "        return spec.float(), label\n",
    "\n",
    "train_ds = InstrumentDataset(train_paths, train_labels)\n",
    "val_ds   = InstrumentDataset(val_paths,   val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch shape:\", x.shape)\n",
    "print(\"Labels:\", y[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T12:14:10.409415Z",
     "iopub.status.busy": "2025-12-19T12:14:10.408570Z",
     "iopub.status.idle": "2025-12-19T12:14:10.414195Z",
     "shell.execute_reply": "2025-12-19T12:14:10.413581Z",
     "shell.execute_reply.started": "2025-12-19T12:14:10.409380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_to_class: {0: 'Accordion', 1: 'Acoustic_Guitar', 2: 'Banjo', 3: 'Bass_Guitar', 4: 'Clarinet', 5: 'Cymbals', 6: 'Dobro', 7: 'Drum_set', 8: 'Electro_Guitar', 9: 'Floor_Tom', 10: 'Harmonica', 11: 'Harmonium', 12: 'Hi_Hats', 13: 'Horn', 14: 'Keyboard', 15: 'Mandolin', 16: 'Organ', 17: 'Piano', 18: 'Saxophone', 19: 'Shakers', 20: 'Tambourine', 21: 'Trombone', 22: 'Trumpet', 23: 'Ukulele', 24: 'Violin', 25: 'cowbell', 26: 'flute', 27: 'vibraphone'}\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(class_to_idx)\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "print(\"idx_to_class:\", idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T12:22:32.468070Z",
     "iopub.status.busy": "2025-12-19T12:22:32.467416Z",
     "iopub.status.idle": "2025-12-19T12:22:32.472201Z",
     "shell.execute_reply": "2025-12-19T12:22:32.471335Z",
     "shell.execute_reply.started": "2025-12-19T12:22:32.468041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T12:26:29.653922Z",
     "iopub.status.busy": "2025-12-19T12:26:29.653069Z",
     "iopub.status.idle": "2025-12-19T12:26:29.659094Z",
     "shell.execute_reply": "2025-12-19T12:26:29.658367Z",
     "shell.execute_reply.started": "2025-12-19T12:26:29.653882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NUM_CLASSES: 28\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Choose GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Also make sure NUM_CLASSES is defined\n",
    "NUM_CLASSES = len(class_to_idx)   # if you built class_to_idx earlier\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "# Training hyperparameters\n",
    "LR = 1e-3          # learning rate\n",
    "NUM_EPOCHS = 10    # or any value you want\n",
    "BATCH_SIZE = 32    # make sure this matches your DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T12:26:34.394311Z",
     "iopub.status.busy": "2025-12-19T12:26:34.393600Z",
     "iopub.status.idle": "2025-12-19T12:26:37.038251Z",
     "shell.execute_reply": "2025-12-19T12:26:37.037524Z",
     "shell.execute_reply.started": "2025-12-19T12:26:34.394279Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=28, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64x64\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 32x32\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN(NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-19T14:06:51.009Z",
     "iopub.execute_input": "2025-12-19T12:39:40.935485Z",
     "iopub.status.busy": "2025-12-19T12:39:40.934746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: train_loss=0.3258 acc=0.9045 | val_loss=0.2433 acc=0.9219\n",
      "  → New best model saved with val_acc: 0.9218953089920832\n",
      "Epoch 02: train_loss=0.2215 acc=0.9333 | val_loss=0.2236 acc=0.9253\n",
      "  → New best model saved with val_acc: 0.9253219898381189\n",
      "Epoch 03: train_loss=0.1726 acc=0.9471 | val_loss=0.1342 acc=0.9602\n",
      "  → New best model saved with val_acc: 0.9601796053408956\n",
      "Epoch 04: train_loss=0.1368 acc=0.9583 | val_loss=0.1524 acc=0.9526\n",
      "Epoch 05: train_loss=0.1222 acc=0.9620 | val_loss=0.1055 acc=0.9680\n",
      "  → New best model saved with val_acc: 0.967978258300839\n",
      "Epoch 06: train_loss=0.1019 acc=0.9688 | val_loss=0.2273 acc=0.9259\n",
      "Epoch 07: train_loss=0.0930 acc=0.9717 | val_loss=0.0858 acc=0.9751\n",
      "  → New best model saved with val_acc: 0.9750679428098783\n",
      "Epoch 08: train_loss=0.0816 acc=0.9744 | val_loss=0.1113 acc=0.9660\n",
      "Epoch 09: train_loss=0.0726 acc=0.9773 | val_loss=0.1219 acc=0.9664\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(loader, model, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total_count += xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    acc = total_correct / total_count\n",
    "    return avg_loss, acc\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, model, optimizer)\n",
    "    val_loss, val_acc = run_epoch(val_loader, model, optimizer=None)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}: \"\n",
    "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"  → New best model saved with val_acc:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-19T14:06:51.014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save final PyTorch model\n",
    "torch.save(model.state_dict(), \"nsynth_instrument_family_cnn.pth\")\n",
    "\n",
    "# Export to ONNX\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 1, N_MELS, SPEC_LEN, device=device)\n",
    "\n",
    "onnx_path = \"nsynth_instrument_family_cnn.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
    "    opset_version=17,\n",
    ")\n",
    "print(\"ONNX model exported to\", onnx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-19T14:06:51.011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "class NsynthOnnxWrapper:\n",
    "    def __init__(self, onnx_model_path):\n",
    "        self.session = ort.InferenceSession(\n",
    "            onnx_model_path,\n",
    "            providers=[\"CPUExecutionProvider\"]\n",
    "        )\n",
    "        self.input_name = self.session.get_inputs()[0].name\n",
    "        self.output_name = self.session.get_outputs()[0].name\n",
    "\n",
    "        # Reuse same transforms as Dataset\n",
    "        self.mel = MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            n_mels=N_MELS,\n",
    "            center=True,\n",
    "            power=2.0,\n",
    "        )\n",
    "        self.to_db = AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "    def preprocess_wav(self, wav_path):\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "            waveform = torchaudio.functional.resample(\n",
    "                waveform, orig_freq=sr, new_freq=SAMPLE_RATE\n",
    "            )\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        spec = self.mel(waveform)\n",
    "        spec = self.to_db(spec).squeeze(0)\n",
    "        # same length handling as Dataset\n",
    "        if spec.size(1) < SPEC_LEN:\n",
    "            pad = SPEC_LEN - spec.size(1)\n",
    "            spec = torch.nn.functional.pad(spec, (0, pad))\n",
    "        elif spec.size(1) > SPEC_LEN:\n",
    "            spec = spec[:, :SPEC_LEN]\n",
    "\n",
    "        mean = spec.mean()\n",
    "        std = spec.std() + 1e-6\n",
    "        spec = (spec - mean) / std\n",
    "        spec = spec.unsqueeze(0).unsqueeze(0)  # (1,1,128,128)\n",
    "\n",
    "        return spec.numpy().astype(\"float32\")\n",
    "\n",
    "    def predict(self, wav_path):\n",
    "        x = self.preprocess_wav(wav_path)\n",
    "        logits = self.session.run([self.output_name], {self.input_name: x})[0]\n",
    "        probs = torch.softmax(torch.from_numpy(logits), dim=1).numpy()[0]\n",
    "        pred_idx = int(np.argmax(probs))\n",
    "        pred_name = INSTR_FAMILY_MAP[pred_idx]\n",
    "        return pred_idx, pred_name, probs\n",
    "\n",
    "#Example usage (outside Streamlit):\n",
    "wrapper = NsynthOnnxWrapper(\"nsynth_instrument_family_cnn.onnx\")\n",
    "idx, name, probs = wrapper.predict(\"/path/to/some.wav\")\n",
    "print(idx, name)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5548421,
     "sourceId": 9179988,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
